my_data <- tibble(x1 = rnorm(30),
x2 = rnorm(30),
x3_noise = rnorm(30),
x4_noise = rnorm(30),
x5_noise = rnorm(30),
error = rnorm(30, sd = 3))
# Simulate data y with underlying population parameter \beta = (3, 1)
my_data <- my_data %>% mutate(y = x1*3 + x2*1 + error)
# Regress y on x1 and x2, without intercept
lm_small <- lm(y ~ x1 + x2 - 1, data = my_data)
# Have a look at the coefficients and summary statistics of the small model
lm_small %>% tidy()   # same as tidy(lm_small)
lm_small %>% glance() # same as glance(lm_small)
# Regress y on x1 and x2 and the noise variables, without intercept
lm_big <- lm(y ~ x1 + x2 + x3_noise + x4_noise + x5_noise - 1, data = my_data)
# Have a look at the coefficients and summary statistics of the small model
lm_big %>% tidy()
lm_big %>% glance()
# Use the multivariate normal distribution to generate 2 variables with 200 observations, generated from a multivariate normal distribution with unit variance an correlation -0.7
# Have a look at ?rmvnorm
?rmvnorm
# Use the multivariate normal distribution to generate 2 variables with 200 observations, generated from a multivariate normal distribution with unit variance an correlation -0.7
# Have a look at ?rmvnorm
help rmvnorm
# This exercise explores the effects of omitted variables
# Replace all instances of "???" to run the code
# Install and attach packages ####
rm(list = ls())
pkgs <- c("tidyverse",
"mvtnorm")
for (i in seq_along(pkgs)){
if (!requireNamespace(pkgs[i], quietly = TRUE)){
install.packages(pkgs[i])
}
}
library(tidyverse)
library(broom)
library(mvtnorm)
# __Set random seed such that the results are reproducible ####
set.seed(1985)
# Use the multivariate normal distribution to generate regressors
# Have a look at ?rmvnorm
?rmvnorm
# Use the multivariate normal distribution to generate regressors
# Have a look at ?rmvnorm
regressors_pos <- rmvnorm(200, sigma = matrix(c(1, 0.7, 0.7, 1), nrow = 2)) %>% as_tibble()
# Simulate the (in reality unobserved) errors and the left-hand-side variable y
# the true underlying population parameters are \beta = (\beta_1, \beta_2)' = (3,2) and \sigma^2 = 1.5^2
data_pos <- regressors_pos %>%
mutate(errors = rnorm(200, mean = 0, sd = 1.5)) %>%
mutate(y = V1*3 + V2*2 + errors)
# __Estimate different models ####
# ____Big model ####
# Estimate the model with the full set of regressors
lmp_big <- lm(y ~ V1 + V2 - 1, data = data_pos)
lmp_big %>% glance()
lmp_big %>% tidy()
# ____Small model ####
# Estimate the models when the second variable is omitted
lmp_small <- lm(y ~ V1 - 1, data = data_pos)
lmp_small %>% glance()
lmp_small %>% tidy()
# Negative correlation between regressors ####
# __Set random seed such that the results are reproducible ####
set.seed(1985)
# __Generate data ####
# Use the multivariate normal distribution to generate 2 variables with 200 observations, generated from a multivariate normal distribution with unit variance an correlation -0.7
# Have a look at ?rmvnorm
regressors_neg <- rmvnorm(200, sigma = matrix(c(1, -0.7, -0.7, 1), nrow = 2)) %>% as_tibble()
# Simulate the (in reality unobserved) errors and the left-hand-side variable y
# the true underlying population parameters are \beta = (\beta_1, \beta_2)' = (3,2) and \sigma^2 = 1.5^2
data_neg <- regressors_neg %>%
mutate(errors = rnorm(200, mean = 0, sd = 1.5)) %>%
mutate(y = V1*3 + V2*2 + errors)
# __Estimate different models ####
# ____Big model ####
lmn_big <- lm(y ~ V1 + V2 - 1, data = data_neg)
lmn_big %>% glance()
lmn_big %>% tidy()
# ____Small model ####
lmn_small <- lm(y ~ V1 - 1, data = data_neg)
lmn_small %>% glance()
lmn_small %>% tidy()
# This exercise explores the effects of omitted variables
# Replace all instances of "???" to run the code
# Install and attach packages ####
rm(list = ls())
pkgs <- c("tidyverse",
"mvtnorm")
for (i in seq_along(pkgs)){
if (!requireNamespace(pkgs[i], quietly = TRUE)){
install.packages(pkgs[i])
}
}
library(tidyverse)
library(broom)
library(mvtnorm)
# Positive correlation between regressors ####
# __Set random seed such that the results are reproducible ####
set.seed(1985)
# __Generate data ####
# Use the multivariate normal distribution to generate regressors
# Have a look at ?rmvnorm
regressors_pos <- rmvnorm(200, sigma = matrix(c(1, 0.7, 0.7, 1), nrow = 2)) %>% as_tibble()
# Simulate the (in reality unobserved) errors and the left-hand-side variable y
# the true underlying population parameters are \beta = (\beta_1, \beta_2)' = (3,2) and \sigma^2 = 1.5^2
data_pos <- regressors_pos %>%
mutate(errors = rnorm(200, mean = 0, sd = 1.5)) %>%
mutate(y = V1*3 + V2*2 + errors)
# __Estimate different models ####
# ____Big model ####
# Estimate the model with the full set of regressors
lmp_big <- lm(y ~ V1 + V2 - 1, data = data_pos)
lmp_big %>% glance()
lmp_big %>% tidy()
# ____Small model ####
# Estimate the models when the second variable is omitted
lmp_small <- lm(y ~ V1 - 1, data = data_pos)
lmp_small %>% glance()
lmp_small %>% tidy()
# Negative correlation between regressors ####
# __Set random seed such that the results are reproducible ####
set.seed(1985)
# __Generate data ####
# Use the multivariate normal distribution to generate 2 variables with 200 observations, generated from a multivariate normal distribution with unit variance an correlation -0.7
# Have a look at ?rmvnorm
regressors_neg <- rmvnorm(200, sigma = matrix(c(1, -0.7, -0.7, 1), nrow = 2)) %>% as_tibble()
# Simulate the (in reality unobserved) errors and the left-hand-side variable y
# the true underlying population parameters are \beta = (\beta_1, \beta_2)' = (3,2) and \sigma^2 = 1.5^2
data_neg <- regressors_neg %>%
mutate(errors = rnorm(200, mean = 0, sd = 1.5)) %>%
mutate(y = V1*3 + V2*2 + errors)
# __Estimate different models ####
# ____Big model ####
lmn_big <- lm(y ~ V1 + V2 - 1, data = data_neg)
lmn_big %>% glance()
lmn_big %>% tidy()
# ____Small model ####
lmn_small <- lm(y ~ V1 - 1, data = data_neg)
lmn_small %>% glance()
lmn_small %>% tidy()
v1 <- c(-1, 0, 1)
selection_vector <- v1 > 0
v1pos <- v1[selection_vector]
v1pos
v1 <- c(1, 2)
v2 <- c(3, 4)
v3 <- (v1, v2)
v3
v1 <- c(1, 2)
v2 <- c(3, 4)
v3 <- c(v1, v2)
v3
matrix1 <- matrix(1:4, byrow = T, nrow = 2)
ctitles <- c("a", "b")
rtitles <- c("c", "d")
# Name the columns with region
colnames(matrix1) <- ctitles
rownames(matrix1) <- rtitles
matrix1
v1 <- c(0, 1)
v2 <- c(0, 0)
matrix1 <- cbind(v1, v2)
matrix1
v1 <- c(0, 1)
v2 <- c(0, 0)
matrix1 <- rbind(v1, v2)
matrix1
v2 <- c(0, 0)
old_string <- "hello"
new_string <- paste(old_string, "there", sep = "_")
new_string
setwd("C:/Users/Teppo/Documents/GitHub/IODS-project/data")
smat <- read.csv("student-mat.csv")
spor <- read.csv("student-por.csv")
str(smat)
head(smat)
smat <- read.csv("student-mat.csv", sep = ",")
str(smat)
smat <- read.csv("student-mat.csv", sep = ".")
str(smat)
##########################################################################################
# Q3. Reading and exploring data
setwd("C:/Users/Teppo/Documents/GitHub/IODS-project/data")
smat <- read.csv("student-mat.csv", sep = "\")
spor <- read.csv("student-por.csv")
str(smat)
head(smat)
smat <- read.csv("student-mat.csv", sep = "\t")
str(smat)
smat <- read.table("student-mat.csv", sep = "\t")
str(smat)
smat <- read.table("student-mat.csv", sep = ",")
str(smat)
smat <- read.table("student-mat.csv", sep = ",", header = T)
str(smat)
smat <- read.table("student-mat.csv", sep = "\t", header = T)
str(smat)
smat <- read.table("student-mat.csv", sep = ",", header = T)
str(smat)
smat <- read_csv("student-mat.csv")
library(readr)
smat <- read_csv("student-mat.csv")
str(smat)
smat <- read_tsv("student-mat.csv")
str(smat)
smat <- read_delim("student-mat.csv", sep = ",")
str(smat)
smat <- read_delim("student-mat.csv", delim = ",")
str(smat)
smat <- read_delim("student-mat.csv", delim = "\t")
str(smat)
smat <- fread("student-mat.csv")
library(fread)
library(data.table)
smat <- fread("student-mat.csv")
str(smat)
spor <- fread("student-por.csv")
head(smat)
str(spor)
##########################################################################################
# Q4. Joining the datasets
id <- c("school", "sex", "age", "address", "famsize", "Pstatus", "Medu",
"Fedu", "Mjob", "Fjob", "reason", "nursery", "internet")
setwd("C:/Users/Teppo/Documents/GitHub/IODS-project/data")
library(data.table)
smat <- fread("student-mat.csv")
spor <- fread("student-por.csv")
str(smat)
str(spor)
# Student-mat dataset includes overall 395 observations and 33 variables, while
# student-por dataset consists of 649 observations and 33 variables.
##########################################################################################
# Q4. Joining the datasets
id <- c("school", "sex", "age", "address", "famsize", "Pstatus", "Medu",
"Fedu", "Mjob", "Fjob", "reason", "nursery", "internet")
jdata <- inner_join(smat, spor, by = id)
library(dplyr)
jdata <- inner_join(smat, spor, by = id)
str(jdata)
notjoinedcols <- colnames(smat)[!colnames(smat) %in% id]
notjoinedcols
alc <- select(jdata, one_of(id))
for(column_name in notjoinedcols) {
two_columns <- select(jdata, starts_with(column_name))
first_column <- select(two_columns, 1)[[1]]
if(is.numeric(first_column)) {
alc[column_name] <- round(rowMeans(two_columns))
} else {
alc[column_name] <- first_column
}
}
str(alc)
glimpse(alc)
alc %>%
mutate(alc_use = (Dalc + Walc) / 2) %>%
mutate(high_use = alc_use > 2)
str(alc)
alc <- alc %>%
mutate(alc_use = (Dalc + Walc) / 2) %>%
mutate(high_use = alc_use > 2)
str(alc)
##########################################################################################
# Q6. Crearing variables alc_use and high_use
glimpse(alc)
write.csv(alc, "alc.csv")
alc <- read_csv("alc.csv", header = T)
alc <- read_csv("alc.csv")
library(readr)
alc <- read_csv("alc.csv")
setwd("C:/Users/Teppo/Documents/GitHub/IODS-project/data")
alc <- read.csv("alc.csv")
alc <- read.csv("alc.csv", row.names = 1)
colnames(alc)
str(alc)
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", header=T, sep="\t")
#dimensions of the data (rows and columns)
dim(df)
#rescale attitude
lrn14$attitude <- df$Attitude / 10
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", header=T, sep="\t")
#dimensions of the data (rows and columns)
dim(df)
#structure of the data
str(df)
# explore the first rows of the data
head(df)
#summarize each variable in the data
summary(df)
#activate dplyr library
library(dplyr)
#learn how to use dplyrs select
?select
#rescale attitude
lrn14$attitude <- df$Attitude / 10
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
dim(lrn14)  # Variable lrn14 has 183 objects of 60 variables
str(lrn14)  # Almost all variables are integers, only 'gender' is a factor with two levels "F" for female and "M" for male.
library(dplyr)  # load dplyr
# create column 'attitude' by scaling the column "Attitude"
lrn14$attitude <- lrn14$Attitude / 10
# questions related to deep, surface and strategic learning
deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D06",  "D15", "D23", "D31")
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
# select the columns related to deep learning and create column 'deep' by averaging
deep_columns <- select(lrn14, one_of(deep_questions))
lrn14$deep <- rowMeans(deep_columns)
# select the columns related to surface learning and create column 'surf' by averaging
surface_columns <- select(lrn14, one_of(surface_questions))
lrn14$surf <- rowMeans(surface_columns)
# select the columns related to strategic learning and create column 'stra' by averaging
strategic_columns <- select(lrn14, one_of(strategic_questions))
lrn14$stra <- rowMeans(strategic_columns)
### Remove extra data from table
keep_columns <- c("gender","Age","attitude", "deep", "stra", "surf", "Points")
# select the 'keep_columns' to create a new dataset
learning2014 <- select(lrn14, one_of(keep_columns))
### Rename columns
colnames(learning2014)[2] <- "age"
# change the name of "Points" to "points"
colnames(learning2014)[7] <- "points"
### Filter out the zero scores
learning2014 <- filter(learning2014, points > 0)
summary(learning2014)  # See that everything is fine.
learning2014 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
str(learning2014)
dim(learning2014)
library(dplyr)
# Then we will create sum composite variables for deep-, surface- and strategic
# learning questions.
deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D06",  "D15", "D23", "D31")
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
deep_columns <- select(learning2014, one_of(deep_questions))
deep <- rowMeans(deep_columns)
surface_columns <- select(learning2014, one_of(surface_questions))
surf <- rowMeans(surface_columns)
strategic_columns <- select(learning2014, one_of(strategic_questions))
stra <- rowMeans(strategic_columns)
# We will pick and name the desired vectors from the dataset
gender <- learning2014$gender
age <- learning2014$Age
attitude <- learning2014$Attitude
points <- learning2014$Points
# Then we will combine the analysis dataset out of our desired components.
learning2014analysis <- data.frame(gender, age, attitude, deep, stra, surf, points)
# We will further exclude observations where the exam points variable is 0.
learning2014analysis <- filter(learning2014analysis, points > 0)
# Now we will revisit the structure and dimensions of our learning2014analysis
# dataset consisting of 7 rows and 166 observations.
str(learning2014analysis)
dim(learning2014analysis)
alc <- read.csv("alc.csv", row.names = 1)
setwd("C:/Users/Teppo/Documents/GitHub/IODS-project/data")
alc <- read.csv("alc.csv", row.names = 1)
colnames(alc)
str(alc)
cor(alc$medu, alc$high_use)
#
alc$high_use <- as.numeric(alc$high_use)
cor(alc$medu, alc$high_use)
cor(alc$Medu, alc$high_use)
setwd("C:/Users/Teppo/Documents/GitHub/IODS-project/data")
alc <- read.csv("alc.csv", row.names = 1)
colnames(alc)
str(alc)
cor(alc$Medu, alc$high_use)
cor(alc$high_use, alc$studytime)
cor(alc$high_use, alc$activities)
alc$activities <- as.factor(alc$activities)
cor(alc$high_use, alc$activities)
alc$activities <- as.numeric(alc$activities)
alc <- read.csv("alc.csv", row.names = 1)
setwd("C:/Users/Teppo/Documents/GitHub/IODS-project/data")
alc <- read.csv("alc.csv", row.names = 1)
colnames(alc)
str(alc)
alc$activities <- as.numeric(alc$activities)
cor(alc$high_use, alc$activities)
cor(alc$high_use, alc$Pstatus)
alc$activities <- as.numeric(alc$Pstatus)
cor(alc$high_use, alc$Pstatus)
setwd("C:/Users/Teppo/Documents/GitHub/IODS-project/data")
alc <- read.csv("alc.csv", row.names = 1)
colnames(alc)
str(alc)
alc$activities <- as.numeric(alc$activities)
alc$Pstatus <- as.numeric(alc$Pstatus)
cor(alc$high_use, alc$Pstatus)
g1 <- ggplot(aes(y = high_use, x = Medu))
library(dplyr)
alc %>%
g1 <- ggplot(aes(y = high_use, x = Medu))
g1 <- ggplot(data = alc, aes(y = high_use, x = Medu))
g1 <- ggplot(alc, aes(y = high_use, x = Medu))
library(tidyr)
g1 <- ggplot(alc, aes(y = high_use, x = Medu))
library(ggplot2)
g1 <- ggplot(alc, aes(y = high_use, x = Medu))
g1 + geom_point()
setwd("C:/Users/Teppo/Documents/GitHub/IODS-project/data")
alc <- read.csv("alc.csv", row.names = 1)
colnames(alc)
str(alc)
alc$activities <- as.numeric(alc$activities)
alc$Pstatus <- as.numeric(alc$Pstatus)
cor(alc$high_use, alc$Medu)
cor(alc$high_use, alc$studytime)
cor(alc$high_use, alc$activities)
cor(alc$high_use, alc$Pstatus)
library(dplyr)
library(ggplot2)
g1 <- ggplot(alc, aes(y = high_use, x = Medu))
g1 + geom_point()
g1 <- ggplot(alc, aes(x = Medu))
g1 + geom_boxplot()
g1 <- ggplot(alc, aes(y = Medu))
g1 + geom_boxplot()
g1 <- ggplot(alc, aes(y = Medu))
g1 + geom_bar()
g1 <- ggplot(alc, aes(x = Medu))
g1 + geom_bar()
g1 <- ggplot(alc, aes(x = Medu))
g1 + geom_col()
g1 <- ggplot(alc, aes(y = Medu))
g1 + geom_col()
g1 <- ggplot(alc, aes(y = Medu))
g1 + geom_bar()
g1 <- ggplot(alc, aes(x = Medu))
g1 + geom_bar()
g1 <- ggplot(alc, aes(x = Medu))
g3 <- ggplot(alc, aes(x = activities))
g4 <- ggplot(alc, aes(x = Pstatus))
g2 + geom_bar()
g2 <- ggplot(alc, aes(x = studytime))
g2 + geom_bar()
g3 + geom_bar()
g4 + geom(bar)
g4 + geom_bar(bar)
g4 + geom_bar()
# Cross tabulations
table(alc$high_use, alc$Medu)
# Cross tabulations
table(alc$high_use, alc$Medu, alc$Pstatus)
# Cross tabulations
table(alc$high_use, alc$Medu)
# Distributional plots
g1 <- ggplot(alc, aes(x = Medu, color = high_use))
g1 + geom_bar()
g1 + geom_col()
# Distributional plots
g1 <- ggplot(alc, aes(y = Medu, color = high_use))
g1 + geom_col()
# Distributional plots
g1 <- ggplot(alc, aes(y = Medu, x = high_use))
g1 + geom_col()
# Distributional plots
g1 <- ggplot(alc, aes(x = Medu))
g1 + geom_bar() + facet_wrap(~high_use)
g1 + geom_histogram() + facet_grid(~high_use)
# Distributional plots
g1 <- ggplot(alc, aes(x = Medu, fill = high_use))
g1 + geom_bar(position = fill)
g1 + geom_bar(position = "fill")
g1 + geom_bar()
# Distributional plots
g1 <- ggplot(alc, aes(x = Medu))
g1 + geom_bar() + facet_wrap(~high_use)
g2 + geom_bar() + facet_wrap(~high_use)
g3 + geom_bar() + facet_wrap(~high_use)
g4 + geom_bar() + facet_wrap(~high_use)
# Correlations visually
pairs(alc)
# Correlations visually
pairs(alc$high_use, alc$Medu)
g1 + geom_bar()
g2 + geom_bar()
g3 + geom_bar()
g4 + geom_bar()
g1 + geom_bar(position = "fill")
# Distributional plots
g1 <- ggplot(alc, aes(x = Medu, fill = high_use))
g1 + geom_bar(position = "fill")
g2 <- ggplot(alc, aes(x = studytime, fill = high_use))
g3 <- ggplot(alc, aes(x = activities, fill = high_use))
g4 <- ggplot(alc, aes(x = Pstatus, fill = high_use))
g2 + geom_bar(position = "fill")
# Distributional plots
g1 <- ggplot(alc, aes(x = Medu))
g2 <- ggplot(alc, aes(x = studytime))
g3 <- ggplot(alc, aes(x = activities))
g4 <- ggplot(alc, aes(x = Pstatus))
g1 + geom_bar()
g2 + geom_bar()
# Distributional plots
g1 <- ggplot(alc, aes(x = Medu))
g1 + geom_bar()
g2 + geom_bar()
g3 + geom_bar()
g4 + geom_bar()
# Simple correlation coefficients
cor(alc$high_use, alc$Medu)
cor(alc$high_use, alc$studytime)
cor(alc$high_use, alc$activities)
cor(alc$high_use, alc$Pstatus)
m1 <- gml(high_use ~ Medu + studytime + activities + Pstatus, data = alc, family = binomial)
m1 <- glm(high_use ~ Medu + studytime + activities + Pstatus, data = alc, family = binomial)
summary(m1)
e^-0.59
exp(0.59)
exp(-0.59)
