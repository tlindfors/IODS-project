library(clusterSEs)
?cluster.wild.plm
??cluster.wild.plm
# predict employment levels, cluster on group
require(plm)
data(EmplUK)
22 cluster.wild.plm
emp.1 <- plm(emp ~ wage + log(capital+1), data = EmplUK, model = "within",
index=c("firm", "year"))
install.packages("plm")
install.packages("clusterSEs")
library(clusterSEs)
install.packages("multiwayvcov")
library(lmtest)
data(petersen)
library(haven)
gradulp2 <- read_dta("Opiskelu/Gradu/gradulp2.dta")
View(gradulp2)
library(lmtest)
data(gradulp2)
data("gradulp2")
m1 <- lm(y ~ x, data = gradulp2)
# Cluster by firm with wild bootstrap and custom wild distribution
boot_id <- cluster.boot(m1, gradulp2$id, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
install.packages("plm")
install.packages("multiwayvcov")
library(lmtest)
m1 <- lm(y ~ x, data = gradulp2)
# Cluster by firm with wild bootstrap and custom wild distribution
boot_id <- cluster.boot(m1, gradulp2$id, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
install.packages("cluster.boot")
# Cluster by firm with wild bootstrap and custom wild distribution
boot_id <- cluster.boot(m1, gradulp2$id, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
?cluster.boot
??cluster.boot
library(multiwayvcov)
# Cluster by firm with wild bootstrap and custom wild distribution
boot_id <- cluster.boot(m1, gradulp2$id, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
data(petersen)
m1 <- lm(y ~ x, data = petersen)
# Cluster by firm with wild bootstrap and custom wild distribution
boot_firm2 <- cluster.boot(m1, petersen$firmid, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
coeftest(m1, boot_firm)
coeftest(m1, boot_firm2)
data(gradulp2)
# Cluster by firm with wild bootstrap and custom wild distribution
boot_id <- cluster.boot(m1, gradulp2$id, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
m1 <- lm(y ~ x, data = gradulp2)
# Cluster by firm with wild bootstrap and custom wild distribution
boot_id <- cluster.boot(m1, gradulp2$id, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
# Cluster by firm with wild bootstrap and custom wild distribution
boot_id <- cluster.boot(m1, gradulp2$id, R=100, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
m1 <- lm(lals~lkl, data = gradulp2)
# Cluster by firm with wild bootstrap and custom wild distribution
boot_id <- cluster.boot(m1, gradulp2$id, R=100, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
coeftest(m1, boot_id)
m1 <- lm(lals~lkl+ldens+cltfp+ltradegdp, data = gradulp2)
# Cluster by firm with wild bootstrap and custom wild distribution
boot_id <- cluster.boot(m1, gradulp2$id, R=100, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
coeftest(m1, boot_id)
m2 <- plm(lals~lkl+ldens+cltfp+ltradegdp, data = gradulp2, effect="time", model="within")
boot_id2 <- cluster.boot(m2, gradulp2$id, R=100, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
m2 <- plm(lals~lkl+ldens+cltfp+ltradegdp, data = gradulp2, effect="time", model="within")
install.packages("plm")
library(plm)
m2 <- plm(formula=lals~lkl+ldens+cltfp+ltradegdp, data = gradulp2, effect="time", model="within")
boot_id2 <- cluster.boot(m2, gradulp2$id, R=100, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
m1 <- lm(lals~lkl+ldens+cltfp+ltradegdp+year, data = gradulp2)
# Cluster by firm with wild bootstrap and custom wild distribution
boot_id <- cluster.boot(m1, gradulp2$id, R=100, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
coeftest(m1, boot_id)
m1 <- lm(lals~lkl+ldens+cltfp+ltradegdp+i.year, data = gradulp2)
m1 <- lm(lals~lkl+ldens+cltfp+ltradegdp+factor(id)-1, data = gradulp2)
# Cluster by firm with wild bootstrap and custom wild distribution
boot_id <- cluster.boot(m1, gradulp2$id, R=100, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
m1 <- lm(lals~lkl+ldens+cltfp+ltradegdp+factor("id")-1, data = gradulp2)
m2 <- plm(formula=lals~lkl+ldens+cltfp+ltradegdp, data = gradulp2, model="within", index=c("id", "year"))
boot_id2 <- cluster.boot(m2, gradulp2$id, R=100, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
coeftest(m2, boot_id2)
m2 <- plm(formula=lals~lkl+ldens+cltfp+ltradegdp, data = gradulp2, model="within", index=c("id", "year"))
boot_id2 <- cluster.boot(m2, gradulp2$id, R=100, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
install.packages("clusterses")
library(clusterses)
install.packages("clusterses")
install.packages("clusterSEs")
library(clusterSEs)
############################
m1 <- plm(lals~lkl+ldens, data=gradulp2, model="within", index=c("id", "year"))
############################
m1 <- plm(lals~lkl+ldens, data="gradulp2", model="within", index=c("id", "year"))
#####clusterSE, example
emp.1 <- plm(emp ~ wage + log(capital+1), data = EmplUK, model = "within",
index=c("firm", "year"))
library(clusterSEs)
#####clusterSE, example
emp.1 <- plm(emp ~ wage + log(capital+1), data = EmplUK, model = "within",
index=c("firm", "year"))
#####clusterSE, example
require(plm)
data(EmplUK)
emp.1 <- plm(emp ~ wage + log(capital+1), data = EmplUK, model = "within",
index=c("firm", "year"))
cluster.wild.plm(mod=emp.1, dat=EmplUK, cluster="group", ci.level = 0.95,
boot.reps = 1000, report = TRUE, prog.bar = TRUE)
############################
data(gradulp2)
############################
data("gradulp2")
m1 <- plm(lals~lkl+ldens, data="gradulp2", model="within", index=c("id", "year"))
cluster.wild.plm(mod=m1, dat=gradulp2, cluster="id", ci.level=0.95, boot.reps=100, report=T, prog.bar=T)
m1 <- plm(lals~lkl+ldens, data=gradulp2, model="within", index=c("id", "year"))
cluster.wild.plm(mod=m1, dat=gradulp2, cluster="id", ci.level=0.95, boot.reps=100, report=T, prog.bar=T)
############################
data("gradulp2")
m1 <- plm(lals~lkl+ldens, data=gradulp2, model="within", index=c("id", "year"))
cluster.wild.plm(mod=m1, dat=gradulp2, cluster="id", ci.level=0.95, boot.reps=100, report=T, prog.bar=T)
###########################
data(gradulp2)
m1 <- plm(lals~lkl+ldens, data=gradulp2, model="within", index=c("id", "year"))
cluster.wild.plm(mod=m1, dat=gradulp2, cluster="id", ci.level=0.95, boot.reps=100, report=T, prog.bar=T)
data(EmplUK)
emp.1 <- plm(emp ~ wage + log(capital+1), data = EmplUK, model = "within",
index=c("firm", "year"))
cluster.wild.plm(mod=emp.1, dat=EmplUK, cluster="group", ci.level = 0.95,
boot.reps = 1000, report = TRUE, prog.bar = TRUE)
A <- [3 5 7; 5 13 20.5]
lm_big %>% glance()
if (!requireNamespace("tidyverse", quietly = TRUE)){
install.packages("tidyverse")
}
library(tidyverse)
library(broom)
# set to 33
set.seed(33)
# fill in the missing parameters at ___
# simulate the the normal distribution wie set to sample size 30
help(rnorm)
my_data <- tibble(x1 = rnorm(30),
x2 = rnorm(30),
x3_noise = rnorm(30),
x4_noise = rnorm(30),
x5_noise = rnorm(30),
error = rnorm(30, sd = 3))
# Simulate data y with underlying population parameter \beta = (3, 1)
my_data <- my_data %>% mutate(y = x1*3 + x2*1 + error)
# Regress y on x1 and x2, without intercept
lm_small <- lm(y ~ x1 + x2 - 1, data = my_data)
# Have a look at the coefficients and summary statistics of the small model
lm_small %>% tidy()   # same as tidy(lm_small)
lm_small %>% glance() # same as glance(lm_small)
# Regress y on x1 and x2 and the noise variables, without intercept
lm_big <- lm(y ~ ___ + ___ + ___ + ___ + ___ - ___, data = my_data)
# Have a look at the coefficients and summary statistics of the small model
lm_big %>% tidy()
lm_big %>% glance()
lm_big <- lm(y ~ x1 + x2 + x3_noise + x4_noise + x5_noise - 1, data = my_data)
# Have a look at the coefficients and summary statistics of the small model
lm_big %>% tidy()
lm_big %>% glance()
if (!requireNamespace("tidyverse", quietly = TRUE)){
install.packages("tidyverse")
}
library(tidyverse)
library(broom)
# set to 33
set.seed(33)
# fill in the missing parameters at ___
# simulate the the normal distribution wie set to sample size 30
help(rnorm)
my_data <- tibble(x1 = rnorm(30),
x2 = rnorm(30),
x3_noise = rnorm(30),
x4_noise = rnorm(30),
x5_noise = rnorm(30),
error = rnorm(30, sd = 3))
# Simulate data y with underlying population parameter \beta = (3, 1)
my_data <- my_data %>% mutate(y = x1*3 + x2*1 + error)
# Regress y on x1 and x2, without intercept
lm_small <- lm(y ~ x1 + x2 - 1, data = my_data)
# Have a look at the coefficients and summary statistics of the small model
lm_small %>% tidy()   # same as tidy(lm_small)
lm_small %>% glance() # same as glance(lm_small)
# Regress y on x1 and x2 and the noise variables, without intercept
lm_big <- lm(y ~ x1 + x2 + x3_noise + x4_noise + x5_noise - 1, data = my_data)
# Have a look at the coefficients and summary statistics of the small model
lm_big %>% tidy()
lm_big %>% glance()
# Use the multivariate normal distribution to generate 2 variables with 200 observations, generated from a multivariate normal distribution with unit variance an correlation -0.7
# Have a look at ?rmvnorm
?rmvnorm
# Use the multivariate normal distribution to generate 2 variables with 200 observations, generated from a multivariate normal distribution with unit variance an correlation -0.7
# Have a look at ?rmvnorm
help rmvnorm
# This exercise explores the effects of omitted variables
# Replace all instances of "???" to run the code
# Install and attach packages ####
rm(list = ls())
pkgs <- c("tidyverse",
"mvtnorm")
for (i in seq_along(pkgs)){
if (!requireNamespace(pkgs[i], quietly = TRUE)){
install.packages(pkgs[i])
}
}
library(tidyverse)
library(broom)
library(mvtnorm)
# __Set random seed such that the results are reproducible ####
set.seed(1985)
# Use the multivariate normal distribution to generate regressors
# Have a look at ?rmvnorm
?rmvnorm
# Use the multivariate normal distribution to generate regressors
# Have a look at ?rmvnorm
regressors_pos <- rmvnorm(200, sigma = matrix(c(1, 0.7, 0.7, 1), nrow = 2)) %>% as_tibble()
# Simulate the (in reality unobserved) errors and the left-hand-side variable y
# the true underlying population parameters are \beta = (\beta_1, \beta_2)' = (3,2) and \sigma^2 = 1.5^2
data_pos <- regressors_pos %>%
mutate(errors = rnorm(200, mean = 0, sd = 1.5)) %>%
mutate(y = V1*3 + V2*2 + errors)
# __Estimate different models ####
# ____Big model ####
# Estimate the model with the full set of regressors
lmp_big <- lm(y ~ V1 + V2 - 1, data = data_pos)
lmp_big %>% glance()
lmp_big %>% tidy()
# ____Small model ####
# Estimate the models when the second variable is omitted
lmp_small <- lm(y ~ V1 - 1, data = data_pos)
lmp_small %>% glance()
lmp_small %>% tidy()
# Negative correlation between regressors ####
# __Set random seed such that the results are reproducible ####
set.seed(1985)
# __Generate data ####
# Use the multivariate normal distribution to generate 2 variables with 200 observations, generated from a multivariate normal distribution with unit variance an correlation -0.7
# Have a look at ?rmvnorm
regressors_neg <- rmvnorm(200, sigma = matrix(c(1, -0.7, -0.7, 1), nrow = 2)) %>% as_tibble()
# Simulate the (in reality unobserved) errors and the left-hand-side variable y
# the true underlying population parameters are \beta = (\beta_1, \beta_2)' = (3,2) and \sigma^2 = 1.5^2
data_neg <- regressors_neg %>%
mutate(errors = rnorm(200, mean = 0, sd = 1.5)) %>%
mutate(y = V1*3 + V2*2 + errors)
# __Estimate different models ####
# ____Big model ####
lmn_big <- lm(y ~ V1 + V2 - 1, data = data_neg)
lmn_big %>% glance()
lmn_big %>% tidy()
# ____Small model ####
lmn_small <- lm(y ~ V1 - 1, data = data_neg)
lmn_small %>% glance()
lmn_small %>% tidy()
# This exercise explores the effects of omitted variables
# Replace all instances of "???" to run the code
# Install and attach packages ####
rm(list = ls())
pkgs <- c("tidyverse",
"mvtnorm")
for (i in seq_along(pkgs)){
if (!requireNamespace(pkgs[i], quietly = TRUE)){
install.packages(pkgs[i])
}
}
library(tidyverse)
library(broom)
library(mvtnorm)
# Positive correlation between regressors ####
# __Set random seed such that the results are reproducible ####
set.seed(1985)
# __Generate data ####
# Use the multivariate normal distribution to generate regressors
# Have a look at ?rmvnorm
regressors_pos <- rmvnorm(200, sigma = matrix(c(1, 0.7, 0.7, 1), nrow = 2)) %>% as_tibble()
# Simulate the (in reality unobserved) errors and the left-hand-side variable y
# the true underlying population parameters are \beta = (\beta_1, \beta_2)' = (3,2) and \sigma^2 = 1.5^2
data_pos <- regressors_pos %>%
mutate(errors = rnorm(200, mean = 0, sd = 1.5)) %>%
mutate(y = V1*3 + V2*2 + errors)
# __Estimate different models ####
# ____Big model ####
# Estimate the model with the full set of regressors
lmp_big <- lm(y ~ V1 + V2 - 1, data = data_pos)
lmp_big %>% glance()
lmp_big %>% tidy()
# ____Small model ####
# Estimate the models when the second variable is omitted
lmp_small <- lm(y ~ V1 - 1, data = data_pos)
lmp_small %>% glance()
lmp_small %>% tidy()
# Negative correlation between regressors ####
# __Set random seed such that the results are reproducible ####
set.seed(1985)
# __Generate data ####
# Use the multivariate normal distribution to generate 2 variables with 200 observations, generated from a multivariate normal distribution with unit variance an correlation -0.7
# Have a look at ?rmvnorm
regressors_neg <- rmvnorm(200, sigma = matrix(c(1, -0.7, -0.7, 1), nrow = 2)) %>% as_tibble()
# Simulate the (in reality unobserved) errors and the left-hand-side variable y
# the true underlying population parameters are \beta = (\beta_1, \beta_2)' = (3,2) and \sigma^2 = 1.5^2
data_neg <- regressors_neg %>%
mutate(errors = rnorm(200, mean = 0, sd = 1.5)) %>%
mutate(y = V1*3 + V2*2 + errors)
# __Estimate different models ####
# ____Big model ####
lmn_big <- lm(y ~ V1 + V2 - 1, data = data_neg)
lmn_big %>% glance()
lmn_big %>% tidy()
# ____Small model ####
lmn_small <- lm(y ~ V1 - 1, data = data_neg)
lmn_small %>% glance()
lmn_small %>% tidy()
v1 <- c(-1, 0, 1)
selection_vector <- v1 > 0
v1pos <- v1[selection_vector]
v1pos
v1 <- c(1, 2)
v2 <- c(3, 4)
v3 <- (v1, v2)
v3
v1 <- c(1, 2)
v2 <- c(3, 4)
v3 <- c(v1, v2)
v3
matrix1 <- matrix(1:4, byrow = T, nrow = 2)
ctitles <- c("a", "b")
rtitles <- c("c", "d")
# Name the columns with region
colnames(matrix1) <- ctitles
rownames(matrix1) <- rtitles
matrix1
v1 <- c(0, 1)
v2 <- c(0, 0)
matrix1 <- cbind(v1, v2)
matrix1
v1 <- c(0, 1)
v2 <- c(0, 0)
matrix1 <- rbind(v1, v2)
matrix1
v2 <- c(0, 0)
setwd("C:/Users/Teppo/Documents/GitHub/IODS-project/data")
alc <- read.csv("alc.csv", row.names = 1)
colnames(alc)
str(alc)
alc$activities <- as.numeric(alc$activities)
alc$Pstatus <- as.numeric(alc$Pstatus)
# Simple correlation coefficients
cor(alc$high_use, alc$Medu)
cor(alc$high_use, alc$studytime)
cor(alc$high_use, alc$activities)
cor(alc$high_use, alc$Pstatus)
# Installing relevant packages
library(dplyr)
library(ggplot2)
# Distributional plots
g1 <- ggplot(alc, aes(x = Medu))
g2 <- ggplot(alc, aes(x = studytime))
g3 <- ggplot(alc, aes(x = activities))
g4 <- ggplot(alc, aes(x = Pstatus))
g1 + geom_bar()
g2 + geom_bar()
g3 + geom_bar()
g4 + geom_bar()
# Cross tabulations
table(alc$high_use, alc$Medu)
table(alc$high_use, alc$studytime)
table(alc$high_use, alc$activities)
table(alc$high_use, alc$Pstatus)
m1 <- glm(high_use ~ Medu + studytime + activities + Pstatus, data = alc, family = binomial)
summary(m1)
exp(coef(m1))
exp(confint(m1))
probabilites <- predict(m1, type = "response")
table(high_use = alc$high_use, prediction = probabilities)
probabilities <- predict(m1, type = "response")
table(high_use = alc$high_use, prediction = probabilities)
alc <- mutate(alc, prediction = probability < 0.5)
alc <- mutate(alc, probability = probabilities)
alc <- mutate(alc, prediction = probability > 0.5)
table(high_use = alc$high_use, prediction = alc$prediction)
m2 <- glm(high_use ~ studytime, data = alc, family = binomial)
probabilities <- predict(m2, type = "response")
alc <- mutate(alc, probability = probabilities)
alc <- mutate(alc, prediction = probability > 0.5)
table(high_use = alc$high_use, prediction = alc$prediction)
summarise(alc$prediction)
summary(alc$prediction)
m2 <- glm(high_use ~ studytime, data = alc, family = "binomial")
probabilities <- predict(m2, type = "response")
alc <- mutate(alc, probability = probabilities)
alc <- mutate(alc, prediction = probability > 0.5)
table(high_use = alc$high_use, prediction = alc$prediction)
summary(alc$prediction)
head(alc$prediction)
head(alc$probability)
alc$probability
m2 <- glm(high_use ~ studytime + Medu + activities + Pstatus, data = alc, family = "binomial")
probabilities <- predict(m2, type = "response")
alc <- mutate(alc, probability = probabilities)
alc <- mutate(alc, prediction = probability > 0.5)
table(high_use = alc$high_use, prediction = alc$prediction)
summary(high_use)
summary(alc$high_use)
m3 <- glm(high_use ~ studytime + sex + absences, data = alc, family = binomial)
probabilities2 <- predict(m3, type = "response")
alc <- mutate(alc, probability2 = probabilities2)
alc <- mutate(alc, prediction2 = probability > 0.5)
table(high_use = alc$high_use, prediction = alc$prediction2)
m3 <- glm(high_use ~ studytime + sex + absences, data = alc, family = binomial)
probabilities2 <- predict(m3, type = "response")
alc <- mutate(alc, probability2 = probabilities2)
alc <- mutate(alc, prediction2 = probability2 > 0.5)
table(high_use = alc$high_use, prediction = alc$prediction2)
alc %>%
ggplot(aes(x = probability, y = high_use, col = prediction)) +
geom_point()
table(high_use = alc$high_use, prediction = alc$prediction) %>%
prop.table() %>%
add.margins() %>%
table(high_use = alc$high_use, prediction = alc$prediction) %>%
prop.table() %>%
add.margins()
# Installing relevant packages
library(dplyr)
library(ggplot2)
table(high_use = alc$high_use, prediction = alc$prediction) %>%
prop.table() %>%
add.margins()
table(high_use = alc$high_use, prediction = alc$prediction) %>%
prop.table() %>%
addmargins()
loss_func <- function(class, prob) {
n_wrong <- abs(class - prob) > 0.5
mean(n_wrong)
}
loss_func(class = alc$high_use, prob = 0)
table(high_use = alc$high_use, prediction = alc$prediction) %>%
prop.table() %>%
addmargins()
m3 <- glm(high_use ~ studytime + sex + absences, data = alc, family = binomial)
probabilities2 <- predict(m3, type = "response")
alc <- mutate(alc, probability2 = probabilities2)
alc <- mutate(alc, prediction2 = probability2 > 0.5)
table(high_use = alc$high_use, prediction = alc$prediction2)
table(high_use = alc$high_use, prediction = alc$prediction) %>%
prop.table() %>%
addmargins()
alc %>%
ggplot(aes(x = probability, y = high_use, col = prediction)) +
geom_point()
a <- matrix(2 -3 1; 2 4 1)
a <- matrix(2, -3, 1, 2, 4, 1, nrow = 2)
a <- matrix(2, -3, 1, 2, 4, 1, nrow = 2, ncol = 3)
a <- matrix(c(2, -3, 1, 2, 4, 1), nrow = 2, ncol = 3)
a
a <- matrix(c(2, 2, -3, 4, 1, 1), nrow = 2, ncol = 3)
a
solve(a)
a <- cbind(c(2, -3), c(2, 4))
a
a <- rbind(c(2, -3), c(2, 4))
a
b <- rbind(c(1/12, 0), c(1/12, 0))
c <- rbind(c(2, 2) c(-3, 4))
c <- rbind(c(2, 2), c(-3, 4))
a%*%b%*%c
a
source('~/.active-rstudio-document', echo=TRUE)
b
b <- rbind(c(1/12, 0), c(0, 1/12))
b
a%*%b%*%c
b
a%*%b%*%c
a <- rbind(c(2, -3), c(2, 4))
b <- rbind(c(1/12, 0), c(0, 1/12))
c <- rbind(c(2, 2), c(-3, 4))
a%*%b%*%c
library(MASS)
data("Boston")
str(Boston)
dim(Boston)
library(GGally)
library(ggplot2)
library(GGally)
install.packages(GGally)
install.packages(GGally)
library(GGally)
ggpairs(Boston, mapping = aes(), lower = list(combo = wrap("facethist", bins = 20)))
ggpairs(Boston, mapping = aes(), lower = list(combo = wrap("facethist", bins = 20)))
library(GGally)
library(dplyr)
library(GGally)
install.packages(GGally)
install.packages(GGally)
if(!require(installr)) {
install.packages("installr"); require(installr)}
updateR()
