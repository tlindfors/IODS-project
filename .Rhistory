# Cluster by firm with wild bootstrap and custom wild distribution
boot_id <- cluster.boot(m1, gradulp2$id, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
?cluster.boot
??cluster.boot
library(multiwayvcov)
# Cluster by firm with wild bootstrap and custom wild distribution
boot_id <- cluster.boot(m1, gradulp2$id, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
data(petersen)
m1 <- lm(y ~ x, data = petersen)
# Cluster by firm with wild bootstrap and custom wild distribution
boot_firm2 <- cluster.boot(m1, petersen$firmid, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
coeftest(m1, boot_firm)
coeftest(m1, boot_firm2)
data(gradulp2)
# Cluster by firm with wild bootstrap and custom wild distribution
boot_id <- cluster.boot(m1, gradulp2$id, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
m1 <- lm(y ~ x, data = gradulp2)
# Cluster by firm with wild bootstrap and custom wild distribution
boot_id <- cluster.boot(m1, gradulp2$id, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
# Cluster by firm with wild bootstrap and custom wild distribution
boot_id <- cluster.boot(m1, gradulp2$id, R=100, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
m1 <- lm(lals~lkl, data = gradulp2)
# Cluster by firm with wild bootstrap and custom wild distribution
boot_id <- cluster.boot(m1, gradulp2$id, R=100, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
coeftest(m1, boot_id)
m1 <- lm(lals~lkl+ldens+cltfp+ltradegdp, data = gradulp2)
# Cluster by firm with wild bootstrap and custom wild distribution
boot_id <- cluster.boot(m1, gradulp2$id, R=100, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
coeftest(m1, boot_id)
m2 <- plm(lals~lkl+ldens+cltfp+ltradegdp, data = gradulp2, effect="time", model="within")
boot_id2 <- cluster.boot(m2, gradulp2$id, R=100, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
m2 <- plm(lals~lkl+ldens+cltfp+ltradegdp, data = gradulp2, effect="time", model="within")
install.packages("plm")
library(plm)
m2 <- plm(formula=lals~lkl+ldens+cltfp+ltradegdp, data = gradulp2, effect="time", model="within")
boot_id2 <- cluster.boot(m2, gradulp2$id, R=100, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
m1 <- lm(lals~lkl+ldens+cltfp+ltradegdp+year, data = gradulp2)
# Cluster by firm with wild bootstrap and custom wild distribution
boot_id <- cluster.boot(m1, gradulp2$id, R=100, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
coeftest(m1, boot_id)
m1 <- lm(lals~lkl+ldens+cltfp+ltradegdp+i.year, data = gradulp2)
m1 <- lm(lals~lkl+ldens+cltfp+ltradegdp+factor(id)-1, data = gradulp2)
# Cluster by firm with wild bootstrap and custom wild distribution
boot_id <- cluster.boot(m1, gradulp2$id, R=100, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
m1 <- lm(lals~lkl+ldens+cltfp+ltradegdp+factor("id")-1, data = gradulp2)
m2 <- plm(formula=lals~lkl+ldens+cltfp+ltradegdp, data = gradulp2, model="within", index=c("id", "year"))
boot_id2 <- cluster.boot(m2, gradulp2$id, R=100, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
coeftest(m2, boot_id2)
m2 <- plm(formula=lals~lkl+ldens+cltfp+ltradegdp, data = gradulp2, model="within", index=c("id", "year"))
boot_id2 <- cluster.boot(m2, gradulp2$id, R=100, boot_type = "wild",
wild_type = function() sample(c(-1, 1), 1))
install.packages("clusterses")
library(clusterses)
install.packages("clusterses")
install.packages("clusterSEs")
library(clusterSEs)
############################
m1 <- plm(lals~lkl+ldens, data=gradulp2, model="within", index=c("id", "year"))
############################
m1 <- plm(lals~lkl+ldens, data="gradulp2", model="within", index=c("id", "year"))
#####clusterSE, example
emp.1 <- plm(emp ~ wage + log(capital+1), data = EmplUK, model = "within",
index=c("firm", "year"))
library(clusterSEs)
#####clusterSE, example
emp.1 <- plm(emp ~ wage + log(capital+1), data = EmplUK, model = "within",
index=c("firm", "year"))
#####clusterSE, example
require(plm)
data(EmplUK)
emp.1 <- plm(emp ~ wage + log(capital+1), data = EmplUK, model = "within",
index=c("firm", "year"))
cluster.wild.plm(mod=emp.1, dat=EmplUK, cluster="group", ci.level = 0.95,
boot.reps = 1000, report = TRUE, prog.bar = TRUE)
############################
data(gradulp2)
############################
data("gradulp2")
m1 <- plm(lals~lkl+ldens, data="gradulp2", model="within", index=c("id", "year"))
cluster.wild.plm(mod=m1, dat=gradulp2, cluster="id", ci.level=0.95, boot.reps=100, report=T, prog.bar=T)
m1 <- plm(lals~lkl+ldens, data=gradulp2, model="within", index=c("id", "year"))
cluster.wild.plm(mod=m1, dat=gradulp2, cluster="id", ci.level=0.95, boot.reps=100, report=T, prog.bar=T)
############################
data("gradulp2")
m1 <- plm(lals~lkl+ldens, data=gradulp2, model="within", index=c("id", "year"))
cluster.wild.plm(mod=m1, dat=gradulp2, cluster="id", ci.level=0.95, boot.reps=100, report=T, prog.bar=T)
###########################
data(gradulp2)
m1 <- plm(lals~lkl+ldens, data=gradulp2, model="within", index=c("id", "year"))
cluster.wild.plm(mod=m1, dat=gradulp2, cluster="id", ci.level=0.95, boot.reps=100, report=T, prog.bar=T)
data(EmplUK)
emp.1 <- plm(emp ~ wage + log(capital+1), data = EmplUK, model = "within",
index=c("firm", "year"))
cluster.wild.plm(mod=emp.1, dat=EmplUK, cluster="group", ci.level = 0.95,
boot.reps = 1000, report = TRUE, prog.bar = TRUE)
A <- [3 5 7; 5 13 20.5]
lm_big %>% glance()
if (!requireNamespace("tidyverse", quietly = TRUE)){
install.packages("tidyverse")
}
library(tidyverse)
library(broom)
# set to 33
set.seed(33)
# fill in the missing parameters at ___
# simulate the the normal distribution wie set to sample size 30
help(rnorm)
my_data <- tibble(x1 = rnorm(30),
x2 = rnorm(30),
x3_noise = rnorm(30),
x4_noise = rnorm(30),
x5_noise = rnorm(30),
error = rnorm(30, sd = 3))
# Simulate data y with underlying population parameter \beta = (3, 1)
my_data <- my_data %>% mutate(y = x1*3 + x2*1 + error)
# Regress y on x1 and x2, without intercept
lm_small <- lm(y ~ x1 + x2 - 1, data = my_data)
# Have a look at the coefficients and summary statistics of the small model
lm_small %>% tidy()   # same as tidy(lm_small)
lm_small %>% glance() # same as glance(lm_small)
# Regress y on x1 and x2 and the noise variables, without intercept
lm_big <- lm(y ~ ___ + ___ + ___ + ___ + ___ - ___, data = my_data)
# Have a look at the coefficients and summary statistics of the small model
lm_big %>% tidy()
lm_big %>% glance()
lm_big <- lm(y ~ x1 + x2 + x3_noise + x4_noise + x5_noise - 1, data = my_data)
# Have a look at the coefficients and summary statistics of the small model
lm_big %>% tidy()
lm_big %>% glance()
if (!requireNamespace("tidyverse", quietly = TRUE)){
install.packages("tidyverse")
}
library(tidyverse)
library(broom)
# set to 33
set.seed(33)
# fill in the missing parameters at ___
# simulate the the normal distribution wie set to sample size 30
help(rnorm)
my_data <- tibble(x1 = rnorm(30),
x2 = rnorm(30),
x3_noise = rnorm(30),
x4_noise = rnorm(30),
x5_noise = rnorm(30),
error = rnorm(30, sd = 3))
# Simulate data y with underlying population parameter \beta = (3, 1)
my_data <- my_data %>% mutate(y = x1*3 + x2*1 + error)
# Regress y on x1 and x2, without intercept
lm_small <- lm(y ~ x1 + x2 - 1, data = my_data)
# Have a look at the coefficients and summary statistics of the small model
lm_small %>% tidy()   # same as tidy(lm_small)
lm_small %>% glance() # same as glance(lm_small)
# Regress y on x1 and x2 and the noise variables, without intercept
lm_big <- lm(y ~ x1 + x2 + x3_noise + x4_noise + x5_noise - 1, data = my_data)
# Have a look at the coefficients and summary statistics of the small model
lm_big %>% tidy()
lm_big %>% glance()
# Use the multivariate normal distribution to generate 2 variables with 200 observations, generated from a multivariate normal distribution with unit variance an correlation -0.7
# Have a look at ?rmvnorm
?rmvnorm
# Use the multivariate normal distribution to generate 2 variables with 200 observations, generated from a multivariate normal distribution with unit variance an correlation -0.7
# Have a look at ?rmvnorm
help rmvnorm
# This exercise explores the effects of omitted variables
# Replace all instances of "???" to run the code
# Install and attach packages ####
rm(list = ls())
pkgs <- c("tidyverse",
"mvtnorm")
for (i in seq_along(pkgs)){
if (!requireNamespace(pkgs[i], quietly = TRUE)){
install.packages(pkgs[i])
}
}
library(tidyverse)
library(broom)
library(mvtnorm)
# __Set random seed such that the results are reproducible ####
set.seed(1985)
# Use the multivariate normal distribution to generate regressors
# Have a look at ?rmvnorm
?rmvnorm
# Use the multivariate normal distribution to generate regressors
# Have a look at ?rmvnorm
regressors_pos <- rmvnorm(200, sigma = matrix(c(1, 0.7, 0.7, 1), nrow = 2)) %>% as_tibble()
# Simulate the (in reality unobserved) errors and the left-hand-side variable y
# the true underlying population parameters are \beta = (\beta_1, \beta_2)' = (3,2) and \sigma^2 = 1.5^2
data_pos <- regressors_pos %>%
mutate(errors = rnorm(200, mean = 0, sd = 1.5)) %>%
mutate(y = V1*3 + V2*2 + errors)
# __Estimate different models ####
# ____Big model ####
# Estimate the model with the full set of regressors
lmp_big <- lm(y ~ V1 + V2 - 1, data = data_pos)
lmp_big %>% glance()
lmp_big %>% tidy()
# ____Small model ####
# Estimate the models when the second variable is omitted
lmp_small <- lm(y ~ V1 - 1, data = data_pos)
lmp_small %>% glance()
lmp_small %>% tidy()
# Negative correlation between regressors ####
# __Set random seed such that the results are reproducible ####
set.seed(1985)
# __Generate data ####
# Use the multivariate normal distribution to generate 2 variables with 200 observations, generated from a multivariate normal distribution with unit variance an correlation -0.7
# Have a look at ?rmvnorm
regressors_neg <- rmvnorm(200, sigma = matrix(c(1, -0.7, -0.7, 1), nrow = 2)) %>% as_tibble()
# Simulate the (in reality unobserved) errors and the left-hand-side variable y
# the true underlying population parameters are \beta = (\beta_1, \beta_2)' = (3,2) and \sigma^2 = 1.5^2
data_neg <- regressors_neg %>%
mutate(errors = rnorm(200, mean = 0, sd = 1.5)) %>%
mutate(y = V1*3 + V2*2 + errors)
# __Estimate different models ####
# ____Big model ####
lmn_big <- lm(y ~ V1 + V2 - 1, data = data_neg)
lmn_big %>% glance()
lmn_big %>% tidy()
# ____Small model ####
lmn_small <- lm(y ~ V1 - 1, data = data_neg)
lmn_small %>% glance()
lmn_small %>% tidy()
# This exercise explores the effects of omitted variables
# Replace all instances of "???" to run the code
# Install and attach packages ####
rm(list = ls())
pkgs <- c("tidyverse",
"mvtnorm")
for (i in seq_along(pkgs)){
if (!requireNamespace(pkgs[i], quietly = TRUE)){
install.packages(pkgs[i])
}
}
library(tidyverse)
library(broom)
library(mvtnorm)
# Positive correlation between regressors ####
# __Set random seed such that the results are reproducible ####
set.seed(1985)
# __Generate data ####
# Use the multivariate normal distribution to generate regressors
# Have a look at ?rmvnorm
regressors_pos <- rmvnorm(200, sigma = matrix(c(1, 0.7, 0.7, 1), nrow = 2)) %>% as_tibble()
# Simulate the (in reality unobserved) errors and the left-hand-side variable y
# the true underlying population parameters are \beta = (\beta_1, \beta_2)' = (3,2) and \sigma^2 = 1.5^2
data_pos <- regressors_pos %>%
mutate(errors = rnorm(200, mean = 0, sd = 1.5)) %>%
mutate(y = V1*3 + V2*2 + errors)
# __Estimate different models ####
# ____Big model ####
# Estimate the model with the full set of regressors
lmp_big <- lm(y ~ V1 + V2 - 1, data = data_pos)
lmp_big %>% glance()
lmp_big %>% tidy()
# ____Small model ####
# Estimate the models when the second variable is omitted
lmp_small <- lm(y ~ V1 - 1, data = data_pos)
lmp_small %>% glance()
lmp_small %>% tidy()
# Negative correlation between regressors ####
# __Set random seed such that the results are reproducible ####
set.seed(1985)
# __Generate data ####
# Use the multivariate normal distribution to generate 2 variables with 200 observations, generated from a multivariate normal distribution with unit variance an correlation -0.7
# Have a look at ?rmvnorm
regressors_neg <- rmvnorm(200, sigma = matrix(c(1, -0.7, -0.7, 1), nrow = 2)) %>% as_tibble()
# Simulate the (in reality unobserved) errors and the left-hand-side variable y
# the true underlying population parameters are \beta = (\beta_1, \beta_2)' = (3,2) and \sigma^2 = 1.5^2
data_neg <- regressors_neg %>%
mutate(errors = rnorm(200, mean = 0, sd = 1.5)) %>%
mutate(y = V1*3 + V2*2 + errors)
# __Estimate different models ####
# ____Big model ####
lmn_big <- lm(y ~ V1 + V2 - 1, data = data_neg)
lmn_big %>% glance()
lmn_big %>% tidy()
# ____Small model ####
lmn_small <- lm(y ~ V1 - 1, data = data_neg)
lmn_small %>% glance()
lmn_small %>% tidy()
v1 <- c(-1, 0, 1)
selection_vector <- v1 > 0
v1pos <- v1[selection_vector]
v1pos
v1 <- c(1, 2)
v2 <- c(3, 4)
v3 <- (v1, v2)
v3
v1 <- c(1, 2)
v2 <- c(3, 4)
v3 <- c(v1, v2)
v3
matrix1 <- matrix(1:4, byrow = T, nrow = 2)
ctitles <- c("a", "b")
rtitles <- c("c", "d")
# Name the columns with region
colnames(matrix1) <- ctitles
rownames(matrix1) <- rtitles
matrix1
v1 <- c(0, 1)
v2 <- c(0, 0)
matrix1 <- cbind(v1, v2)
matrix1
v1 <- c(0, 1)
v2 <- c(0, 0)
matrix1 <- rbind(v1, v2)
matrix1
v2 <- c(0, 0)
# Päivämääräksi pakottaminen
dmy("17 Sep 2015")
# Päivämääräksi pakottaminen
library(lubridate)
dmy("17 Sep 2015")
# Puuttivien muuttujien laskeminen
sum(is.na(df))
##########################################################################################
# Reading data
learning2014 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt"
sep = "\")
learning2014 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt"
sep = "\t", header = T)
learning2014 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt",
sep = "\t", header = T)
str(learning2014)
head(learning2014)
library(dplyr)
# Adding a composite index of deep, surface and strategic learning to the dataset
deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D06",  "D15", "D23", "D31")
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
deep_cols <- select(lrn2014, one_of(deep_questions))
lrn2014$deep <- rowMeans(deep_cols)
surf_cols <- select(lrn2014, one_of(surface_questions))
lrn2014$surf <- rowMeans(surf_cols)
stra_cols <- select(lrn2014, one_of(strategic_questions))
lrn2014$stra <- rowMeans(stra_cols)
# Picking variables from the full dataset for further analysis
lrn2014a <- lrn2014 %>%
select(gender, Age, Attitude, deep, surf, stra, Points)
filter(!is.na(Points))
lrn2014 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt",
sep = "\t", header = T)
str(l2014)
head(l2014)
# The data appears to be in the form of a dataframe of dimension 183x60, i.e.,
# it consists of 183 observations and 60 different variables.
##########################################################################################
# Q3. Extracting a dataset for analysis
# Installing necessary packages
library(dplyr)
# Adding a composite index of deep, surface and strategic learning to the dataset
deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D06",  "D15", "D23", "D31")
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
deep_cols <- select(lrn2014, one_of(deep_questions))
lrn2014$deep <- rowMeans(deep_cols)
surf_cols <- select(lrn2014, one_of(surface_questions))
lrn2014$surf <- rowMeans(surf_cols)
stra_cols <- select(lrn2014, one_of(strategic_questions))
lrn2014$stra <- rowMeans(stra_cols)
# Picking variables from the full dataset for further analysis
lrn2014a <- lrn2014 %>%
select(gender, Age, Attitude, deep, surf, stra, Points)
filter(!is.na(Points))
lrn2014 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt",
sep = "\t", header = T)
str(lrn2014)
head(lrn2014)
lrn2014 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt",
sep = "\t", header = T)
str(lrn2014)
head(lrn2014)
# The data appears to be in the form of a dataframe of dimension 183x60, i.e.,
# it consists of 183 observations and 60 different variables.
##########################################################################################
# Q3. Extracting a dataset for analysis
# Installing necessary packages
library(dplyr)
# Adding a composite index of deep, surface and strategic learning to the dataset
deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D06",  "D15", "D23", "D31")
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
deep_cols <- select(lrn2014, one_of(deep_questions))
lrn2014$deep <- rowMeans(deep_cols)
surf_cols <- select(lrn2014, one_of(surface_questions))
lrn2014$surf <- rowMeans(surf_cols)
stra_cols <- select(lrn2014, one_of(strategic_questions))
lrn2014$stra <- rowMeans(stra_cols)
# Picking variables from the full dataset for further analysis
lrn2014a <- lrn2014 %>%
select(gender, Age, Attitude, deep, surf, stra, Points)
filter(!is.na(Points))
lrn2014 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt",
sep = "\t", header = T)
str(lrn2014)
head(lrn2014)
# The data appears to be in the form of a dataframe of dimension 183x60, i.e.,
# it consists of 183 observations and 60 different variables.
##########################################################################################
# Q3. Extracting a dataset for analysis
# Installing necessary packages
library(dplyr)
# Adding a composite index of deep, surface and strategic learning to the dataset
deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D06",  "D15", "D23", "D31")
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
deep_cols <- select(lrn2014, one_of(deep_questions))
lrn2014$deep <- rowMeans(deep_cols)
surf_cols <- select(lrn2014, one_of(surface_questions))
lrn2014$surf <- rowMeans(surf_cols)
stra_cols <- select(lrn2014, one_of(strategic_questions))
lrn2014$stra <- rowMeans(stra_cols)
# Picking variables from the full dataset for further analysis
lrn2014a <- lrn2014 %>%
select(gender, Age, Attitude, deep, surf, stra, Points) %>%
filter(!is.na(Points))
str(lrn2014a)
lrn2014a <- lrn2014 %>%
select(gender, Age, Attitude, deep, surf, stra, Points) %>%
filter(na.omit(Points))
str(lrn2014a)
lrn2014a <- lrn2014 %>%
select(gender, Age, Attitude, deep, surf, stra, Points) %>%
filter(Points != 0)
str(lrn2014a)
##########################################################################################
# Q4. Setting a new working directory
setwd("C:/Users/Teppo/Documents/GitHub/IODS-project")
?write.csv
# Saving the analysis dataset as a csv-file
write.csv(lrn2014a, "lrn2014a.csv")
# Rereading the analysis dataset as a demonstration
read.csv("lrn2014a.csv", header = T)
# Rereading the analysis dataset as a demonstration
lrn2014a_demo <- read.csv("lrn2014a.csv", header = T)
str(lrn2014a_demo)
head(lrn2014a_demo)
# Checking whether the extraction worked
str(lrn2014a)
# Rereading the analysis dataset as a demonstration
lrn2014a_demo <- read.csv("lrn2014a.csv")
str(lrn2014a_demo)
head(lrn2014a_demo)
# Rereading the analysis dataset as a demonstration
lrn2014a_demo <- read.csv("lrn2014a.csv", row.names = 1)
str(lrn2014a_demo)
head(lrn2014a_demo)
# Reading the data
lrn2014a <- read.csv("lrn2014a.csv", row.names = 1)
str(lrn2014a)
head(lrn2014a)
hist(lrn2014a$age)
hist("lrn2014a$age")
lrn2014a$Age <- as.numeric(lrn2014a$Age)
lrn2014a$Attitude <- as.numeric(lrn2014a$Attitude)
lrn2014a$Points <- as.numeric(lrn2014a$Points)
hist(lrn2014a$age)
hist(lrn2014a$Age)
lrn2014a <- read.csv("lrn2014a.csv", row.names = 1)
hist(lrn2014a$Age)
summary(lrn2014a$Age)
hist(lrn2014a$Attitude)
str(lrn2014a_demo)
hist(lrn2014a$deep)
hist(lrn2014a$surf)
hist(lrn2014a$deep)
hist(lrn2014a$stra)
hist(lrn2014a$Points)
summary(lrn2014a$gender)
count(lrn2014a$gender["F"])
sum(lrn2014a$gender["F"])
sum(lrn2014a$gender == F)
sum(lrn2014a$gender == "F")
sum(lrn2014a$gender == "F")/(sum(lrn2014a$gender == "F") + sum(lrn2014a$gender == M))
sum(lrn2014a$gender == "F")/(sum(lrn2014a$gender == "F") + sum(lrn2014a$gender == "M"))
pairs()
pairs(lrn2014a)
corr(lrn2014a$surf, lrn2014a$Age)
r(lrn2014a$surf, lrn2014a$Age)
cor(lrn2014a$surf, lrn2014a$Age)
cor(lrn2014a$Age, lrn2014a$surf)
cor(lrn2014a$Attitude, lrn2014a$Points)
pairs(lrn2014a)
cor(lrn2014a$deep, lrn2014a$surf)
pairs(lrn2014a)
cor(lrn2014a$Age, lrn2014a$deep)
pairs(lrn2014a)
m1 <- lm(points ~ Attitude + Age + deep, data = lrn2014a)
m1 <- lm(points ~ Attitude + Age + deep, data = "lrn2014a")
str(lrn2014a)
m1 <- lm(Points ~ Attitude + Age + deep, data = lrn2014a)
summary(m1)
m2 <- lm(Points ~ Attitude, data = lrn2014a)
summary(m2)
summary(lrn2014a$Attitude)
par(mfrow = c(2, 2))
plot(m2, which = c(1, 2, 5))
par(mfrow = c(2, 2))
plot(m2, which = c(1, 2, 5))
